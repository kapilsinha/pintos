       	       	     +-------------------------+
                     |          CS 124         |
                     | PROJECT 6: FILE SYSTEMS |
                     |     DESIGN DOCUMENT     |
                     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Kapil Sinha <ksinha@caltech.edu>
Karthik Nair <knair@caltech.edu>

>> Specify how many late tokens you are using on this assignment:
1

>> What is the Git repository and commit hash for your submission?

   Repository URL: https://github.com/kapilsinha/pintos
   commit d0f05c5a5aadc43737cc706a4759d4d232b9044f

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

    Note we asked a TA in advance if we can use a late token and he approved.

    We built on project 5 and enabled VM for extra credit. Also, there was
    slight discrepancy in test results between the team members' computers but
    ultimately all the tests except for dir-vine-persistence work on one of
    ours. Since term is ending and we may not have as much access to our
    computers, we will message a video of our code running the tests to the
    TAs for proof.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course instructors.

    N/A

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/*!
 *  On-disk inode.
 *  Must be exactly BLOCK_SECTOR_SIZE bytes long.
 */
struct inode_disk {
    block_sector_t direct[NUM_DIRECT];      /*!< Direct nodes. */
    block_sector_t indirect;                /*!< Indirect nodes. */
    block_sector_t double_indirect;         /*!< Double indirect nodes. */
    unsigned is_dir;                        /*!< 1 if dir, 0 if file. */
    block_sector_t parent_dir_sector;       /*!< Parent dir's inode->sector */
    off_t length;                           /*!< File size in bytes. */
    unsigned magic;                         /*!< Magic number. */
    uint32_t unused[110];                   /*!< Not used. */
};

/*! In-memory inode. */
struct inode {
    struct list_elem elem;            /*!< Element in inode list. */
    block_sector_t sector;            /*!< Sector number of disk location. */
    int open_cnt;                     /*!< Number of openers. */
    bool removed;                     /*!< True if deleted, false otherwise. */
    int deny_write_cnt;               /*!< 0: writes ok, >0: deny writes. */
    struct lock extension_lock;       /*!< Lock used for extending the file. */
};

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

    The maximum size of a file supported by our inode structure is
    16,524 * 512 = 8,460,288 bytes. This is because given our structure, we can
    have 12 direct blocks, 128 indirect blocks, and 128 * 128 double indirect
    blocks which adds up to 16,524.

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

    We added an extension lock to our inode struct which must be acquired
    anytime a process tries to read a file, write to a file, or extend a file.
    This prevents two processes from extending the same file at the same time.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

    We added an extension lock to our inode struct. This lock must be acquired
    for both reads and writes. For reads, the lock is simply acquired to check
    if we are trying to read past the end of file. This keeps A from reading
    while B is extending the file because B holds this extension lock until it
    is completely done extending the file and writing all of its data. Thus the
    lock is not held for long by readers, which easily allows for several
    readers to simultaneously read a file.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

    We implemented our own shared-exclusive lock which is used whenever trying
    to read or write to a cache entry; this struct is called rw_lock and is
    implemented in threads/synch.c. See the Data Structures section under
    Buffer Cache for more detail on this read-write lock.

    Since we have a shared-exclusive lock, readers can never block other
    readers. Also, when a writer releases the lock, it broadcasts to all
    readers to allow them to read. Until this set number of permitted
    readers are done reading, no other writers are allowed access to the lock.
    Similarly, readers may not acquire the lock if there is a writer waiting,
    and the last reader signals the writer to allow it to access the lock.
    This way, neither readers nor writers can get starved, with readers
    allowing writers to access next when the last reader finishes and
    writers allowing readers to access next when the writer finishes.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

    Yes, our inode structure is a multilevel index. We tried to keep our
    implementation as simple as possible and therefore kept the number of
    direct, indirect, and doubly indirect blocks to a minimum. Thus, we used
    12 direct blocks, 1 indirect block, and 1 doubly indirect block. Our
    design was heavily based on the ext2 file system.

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct short_path {
    struct dir *dir;      /* Directory containing the file */
    const char *filename; /* Name of the file the path refers to */
    bool is_dir;          /* True if filename is a dir,
                             false if it is an ordinary file */
};
    The above struct is mainly just used to package information together
    from our parsing method.

struct thread {
    ...
    /*! Used for implementing subdirectories for filesys assignment. */
    /**@{*/
    struct dir *cur_dir;       /*!< Thread's current directory */
    /**@}*/
}
    We associated a cur_dir for each process/thread so it can maintain
    its own current directory.

struct inode_disk {
    block_sector_t direct[NUM_DIRECT];      /*!< Direct nodes. */
    block_sector_t indirect;                /*!< Indirect nodes. */
    block_sector_t double_indirect;         /*!< Double indirect nodes. */
    unsigned is_dir;                        /*!< 1 if dir, 0 if file. */
    block_sector_t parent_dir_sector;       /*!< Parent dir's inode->sector */
    off_t length;                           /*!< File size in bytes. */
    unsigned magic;                         /*!< Magic number. */
    uint32_t unused[110];                   /*!< Not used. */
};
    We added a flag to each inode_disk saying whether it is a directory
    and also store the parent's inode->sector.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

    We immediately determine if the path is an absolute or relative
    path based on whether the path does or does not start with a '/',
    respectively. After that, we consider each portion of the path
    separated by '/'s and update our current directory iteratively.
    If the portion is a '.' or '..', then we stay at the current
    or go to the parent directory (using the parent_dir_sector we
    added to this directory's inode's metadata). Otherwise, we use
    lookup to find a file with the name we are searching for, and if
    the directory exists, we continue iteratively (else we know the
    path is invalid). We linearly scan our path and do linked list
    like manipulations to find our final directory, which is very
    efficient for the task.

    Then at the end, depending on whether the path is invalid or
    refers to an ordinary file or a directory, we return our result
    in short_path, which is sufficient to do all our syscalls.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

    Our functions on dir_entries call the appropriate functions to
    modify the underlying inode objects, which contains most of the
    information we are manipulating. Thus our synchronization for
    the inodes (read/write lock and extension lock) are useful
    to prevent races for directory entries too.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

    Our implementation does allow a directory to be removed if it is
    open by a process or in use as a current working directory. We
    thus handle directories similar to how we handle files, since
    for files too we allow removing a file while it is open by a
    process.

    We only remove a directory, however, if it is empty (which we
    check easily via the in use flag of the dir_entries) and in
    the path parsing method, we consider a path that tries to use
    a removed directory invalid. That is, if a directory has been
    removed and some path attempts to go to it or through it (via
    . or .. especially if a process has it as its current directory),
    we deem the path invalid.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

    We initially thought of storing the current directory as a string,
    but realized this could have issues with memory (a path can be
    very long) and based on the operations we need to perform, inefficient.

    Hence we realized we want to be able to access the inode quickly since
    that allows us to access all the underlying data for the backing data,
    and also maintain a position for use by readdir. The dir struct
    encompassed this very well, so we designed a way to navigate paths
    using these dirs as nodes in a linked list - since we store the parent
    dir of each file/dir, and each dir can access its children via lookup.

    This pseudo-graph based approach allowed us to easily and efficiently
    traverse and search the filesystem.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* A single entry for the cache representing one block. */
struct file_cache_entry {
    /* Sector contains metadata if the sector holds an inode_disk, and
     * contains file data if the sector holds a data (file) sector */
    block_sector_t sector;  /* Sector whose data we hold, key to hash map */
    void *data;             /* Cached data contained at the inode
                             * (originally, contents at inode->sector) */
    bool in_use;            /* 1 if entry is being used (data is valid),
                             * else 0 */
    bool accessed;          /* 1 if data has been accessed, else 0 */
    bool dirty;             /* 1 if data has been written to, else 0 */
    struct rw_lock rw_lock; /* Per cache entry read-write lock */
    struct lock evict_lock; /* TODO: Why do I need this??? */
};

/*! Read-write lock. */
struct rw_lock {
    struct lock lock;          /* Read/write lock */
    int num_readers;           /* Number of readers currently holding lock */
    int num_readers_waiting;   /* Number of readers waiting for the lock */
    int num_writers_waiting;   /* Number of writers waiting for the lock */
    int num_readers_permitted; /* Number of readers to let read (writer should
                                * not acquire the lock until this many readers
                                * have acquired the lock */
    struct condition readers;  /* Contains the list of waiting reader threads */
    struct condition writers;  /* Contains the list of waiting writer threads */
};

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

    We implemented the clock eviction policy to determine which cache entry to
    evict. We initialize the "clock hand" to point to the first entry of the
    cache table. When choosing a policy to evict, we first check the entry
    pointed to by the "clock hand". If it is marked as accessed, we clear it and
    move on to the next entry. Otherwise, we return it.

>> C3: Describe your implementation of write-behind.

    We created a separate kernel thread which loops over the entire cache every
    10 timer ticks and individually writes back each cached block to the disk.
    We make sure to only write back cache blocks that have actually been
    accessed.

>> C4: Describe your implementation of read-ahead.

    We used a list as our queue. Whenever a call to file_cache_read is made, the
    next sector (n + 1) is pushed back to the list. We also created a separate
    kernel thread that then checks this list, pops an entry from the front of
    the list, and adds the data from that sector on disk into the cache if it
    was not already in the cache.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

    Our evict_block function tries to acquire the read/write lock as a writer
    and the eviction lock. This keeps a process from evicting the block while it
    is being accessed because it would get blocked on trying to acquire the
    read/write lock.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

    When the evict_block function is evicting an entry from the cache, it tries
    to acquire the read/write lock as a writer which means that it tries to
    get exclusive access to the entry. This keeps other processes from trying
    to read this entry or write to it.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

    If a process is repeatedly reading and writing to the same part of the file,
    it would benefit from buffer caching because having a buffer cache would
    reduce the number of times the process needs to access the disk. However, if
    instead the process is looping through a fairly large file which has a size
    greater than the buffer cache and reading and writing as it goes, it would
    benefit more from read-ahead and write-behind because reading in the next
    sector into the cache would speed up looping through the file.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the feedback survey on the course
website.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

This assignment was definitely tricky, and unexpectedly so. It seemed like
several parts were implied to be straightforward but weren't - including
the buffer cache and subdirectories!

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

The concept of the buffer cache and implementing that was definitely a
good experience to solidify the concept of maximizing quick operations.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Buffer cache is NOT straightforward. Transitioning to local (not global)
locks can be very tricky. Also, some more guidance for the subdirectories
portion would be nice. It seemed like we had to come up with a design
from scratch, which was quite difficult.

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?

I think it would be better if the amount of points for all of the tests added
up to a 100 or some definitive number because it seems like a lot of points
can be taken off just for failing a single test.
